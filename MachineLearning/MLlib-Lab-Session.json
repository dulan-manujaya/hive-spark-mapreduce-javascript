{"paragraphs":[{"text":"%md \n# Estimator, Transformer, and Param","user":"anonymous","dateUpdated":"2019-12-01T18:02:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Estimator, Transformer, and Param</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326679_780281736","id":"20161129-012735_1377582462","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:238","dateFinished":"2019-12-01T18:02:16+0000","dateStarted":"2019-12-01T18:02:13+0000"},{"text":"%pyspark\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\n\n# Prepare training data from a list of (label, features) tuples.\ntraining = spark.createDataFrame([\n    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n\n# Create a LogisticRegression instance. This instance is an Estimator.\nlr = LogisticRegression(maxIter=10, regParam=0.01)\n# Print out the parameters, documentation, and any default values.\nprint \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\"\n\n# Learn a LogisticRegression model. This uses the parameters stored in lr.\nmodel1 = lr.fit(training)\n\n# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n# we can view the parameters it used during fit().\n# This prints the parameter (name: value) pairs, where names are unique IDs for this\n# LogisticRegression instance.\nprint \"Model 1 was fit using parameters: \"\nprint model1.extractParamMap()\n\n# We may alternatively specify parameters using a Python dictionary as a paramMap\nparamMap = {lr.maxIter: 20}\nparamMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\nparamMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n\n# You can combine paramMaps, which are python dictionaries.\nparamMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\nparamMapCombined = paramMap.copy()\nparamMapCombined.update(paramMap2)\n\n# Now learn a new model using the paramMapCombined parameters.\n# paramMapCombined overrides all parameters set earlier via lr.set* methods.\nmodel2 = lr.fit(training, paramMapCombined)\nprint \"Model 2 was fit using parameters: \"\nprint model2.extractParamMap()\n\n# Prepare test data\ntest = spark.createDataFrame([\n    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n\n# Make predictions on test data using the Transformer.transform() method.\n# LogisticRegression.transform will only use the 'features' column.\n# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n# 'probability' column since we renamed the lr.probabilityCol parameter previously.\nprediction = model2.transform(test)\nselected = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\")\nfor row in selected.collect():\n    print row","user":"anonymous","dateUpdated":"2019-12-01T18:02:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"LogisticRegression parameters:\naggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label)\nmaxIter: max number of iterations (>= 0). (default: 100, current: 10)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n\nModel 1 was fit using parameters: \n{}\nModel 2 was fit using parameters: \n{}\nRow(features=DenseVector([-1.0, 1.5, 1.3]), label=1.0, myProbability=DenseVector([0.0571, 0.9429]), prediction=1.0)\nRow(features=DenseVector([3.0, 2.0, -0.1]), label=0.0, myProbability=DenseVector([0.9239, 0.0761]), prediction=0.0)\nRow(features=DenseVector([0.0, 2.2, -1.5]), label=1.0, myProbability=DenseVector([0.1097, 0.8903]), prediction=1.0)\n"}]},"apps":[],"jobName":"paragraph_1575223326689_56094436","id":"20161129-012748_52787394","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:239","dateFinished":"2019-12-01T18:03:01+0000","dateStarted":"2019-12-01T18:02:17+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=0","http://172.17.0.2:4040/jobs/job?id=1","http://172.17.0.2:4040/jobs/job?id=2","http://172.17.0.2:4040/jobs/job?id=3","http://172.17.0.2:4040/jobs/job?id=4","http://172.17.0.2:4040/jobs/job?id=5","http://172.17.0.2:4040/jobs/job?id=6","http://172.17.0.2:4040/jobs/job?id=7","http://172.17.0.2:4040/jobs/job?id=8","http://172.17.0.2:4040/jobs/job?id=9","http://172.17.0.2:4040/jobs/job?id=10","http://172.17.0.2:4040/jobs/job?id=11","http://172.17.0.2:4040/jobs/job?id=12","http://172.17.0.2:4040/jobs/job?id=13","http://172.17.0.2:4040/jobs/job?id=14","http://172.17.0.2:4040/jobs/job?id=15","http://172.17.0.2:4040/jobs/job?id=16","http://172.17.0.2:4040/jobs/job?id=17","http://172.17.0.2:4040/jobs/job?id=18","http://172.17.0.2:4040/jobs/job?id=19","http://172.17.0.2:4040/jobs/job?id=20","http://172.17.0.2:4040/jobs/job?id=21","http://172.17.0.2:4040/jobs/job?id=22","http://172.17.0.2:4040/jobs/job?id=23","http://172.17.0.2:4040/jobs/job?id=24","http://172.17.0.2:4040/jobs/job?id=25","http://172.17.0.2:4040/jobs/job?id=26","http://172.17.0.2:4040/jobs/job?id=27","http://172.17.0.2:4040/jobs/job?id=28","http://172.17.0.2:4040/jobs/job?id=29","http://172.17.0.2:4040/jobs/job?id=30","http://172.17.0.2:4040/jobs/job?id=31","http://172.17.0.2:4040/jobs/job?id=32","http://172.17.0.2:4040/jobs/job?id=33","http://172.17.0.2:4040/jobs/job?id=34","http://172.17.0.2:4040/jobs/job?id=35","http://172.17.0.2:4040/jobs/job?id=36","http://172.17.0.2:4040/jobs/job?id=37","http://172.17.0.2:4040/jobs/job?id=38","http://172.17.0.2:4040/jobs/job?id=39","http://172.17.0.2:4040/jobs/job?id=40","http://172.17.0.2:4040/jobs/job?id=41","http://172.17.0.2:4040/jobs/job?id=42"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Extracting, transforming and selecting features\n\n## CountVectorizer\n","user":"anonymous","dateUpdated":"2019-12-01T18:08:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Extracting, transforming and selecting features</h1>\n<h2>CountVectorizer</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326690_-1788286712","id":"20161129-013007_1388002982","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:240","dateFinished":"2019-12-01T18:07:57+0000","dateStarted":"2019-12-01T18:07:57+0000"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\nmodel = cv.fit(df)\nresult = model.transform(df)\nresult.show()","user":"anonymous","dateUpdated":"2019-12-01T18:08:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---------------+--------------------+\n| id|          words|            features|\n+---+---------------+--------------------+\n|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|\n|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|\n+---+---------------+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1575223326691_169518940","id":"20161129-013020_1764358722","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:241","dateFinished":"2019-12-01T18:08:04+0000","dateStarted":"2019-12-01T18:08:02+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=43","http://172.17.0.2:4040/jobs/job?id=44","http://172.17.0.2:4040/jobs/job?id=45"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n## Feature Transformers : Tokenizer","user":"anonymous","dateUpdated":"2019-12-01T18:08:42+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575223326691_2139339725","id":"20161129-013707_1456126627","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:242","dateFinished":"2019-12-01T18:08:42+0000","dateStarted":"2019-12-01T18:08:42+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Feature Transformers : Tokenizer</h2>\n</div>"}]}},{"text":"%pyspark \n\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\n\nsentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat\")\n], [\"label\", \"sentence\"])\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsDataFrame = tokenizer.transform(sentenceDataFrame)\nfor words_label in wordsDataFrame.select(\"words\", \"label\").take(3):\n    print(words_label)\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)","user":"anonymous","dateUpdated":"2019-12-01T18:08:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(words=[u'hi', u'i', u'heard', u'about', u'spark'], label=0)\nRow(words=[u'i', u'wish', u'java', u'could', u'use', u'case', u'classes'], label=1)\nRow(words=[u'logistic,regression,models,are,neat'], label=2)\n"}]},"apps":[],"jobName":"paragraph_1575223326692_1467889948","id":"20161129-013723_581839226","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:243","dateFinished":"2019-12-01T18:08:46+0000","dateStarted":"2019-12-01T18:08:45+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=46"],"interpreterSettingId":"spark"}}},{"text":"%md \n## Feature Selectors: VectorSlicer","user":"anonymous","dateUpdated":"2019-12-01T18:08:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575223326693_-1183009064","id":"20161129-013842_1827572139","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:244","dateFinished":"2019-12-01T18:08:50+0000","dateStarted":"2019-12-01T18:08:50+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Feature Selectors: VectorSlicer</h2>\n</div>"}]}},{"text":"%pyspark\n\nfrom pyspark.ml.feature import VectorSlicer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import Row\n\ndf = spark.createDataFrame([\n    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3}),),\n    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]),)])\n\nslicer = VectorSlicer(inputCol=\"userFeatures\", outputCol=\"features\", indices=[1])\n\noutput = slicer.transform(df)\n\noutput.select(\"userFeatures\", \"features\").show()","user":"anonymous","dateUpdated":"2019-12-01T18:08:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+-------------+\n|        userFeatures|     features|\n+--------------------+-------------+\n|(3,[0,1],[-2.0,2.3])|(1,[0],[2.3])|\n|      [-2.0,2.3,0.0]|        [2.3]|\n+--------------------+-------------+\n\n"}]},"apps":[],"jobName":"paragraph_1575223326693_517602419","id":"20161129-013858_179930977","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:245","dateFinished":"2019-12-01T18:08:53+0000","dateStarted":"2019-12-01T18:08:53+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=47"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Classification","user":"anonymous","dateUpdated":"2019-12-01T18:09:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575223326693_-722538628","id":"20161129-005933_456191674","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:246","dateFinished":"2019-12-01T18:09:04+0000","dateStarted":"2019-12-01T18:09:04+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Classification</h1>\n</div>"}]}},{"text":"%md\n\n## Logistic regression\nLogistic regression is a popular method to predict a binary response. It is a special case of Generalized Linear models that predicts the probability of the outcome.","user":"anonymous","dateUpdated":"2019-12-01T18:09:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Logistic regression</h2>\n<p>Logistic regression is a popular method to predict a binary response. It is a special case of Generalized Linear models that predicts the probability of the outcome.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326694_-731955929","id":"20161129-010017_1506647346","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:247","dateFinished":"2019-12-01T18:09:08+0000","dateStarted":"2019-12-01T18:09:08+0000"},{"text":"%pyspark \n\n#The following example shows how to train a logistic regression model with elastic net regularization\n\nfrom pyspark.ml.classification import LogisticRegression\n\n# Load training data\ntraining = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_libsvm_data.txt\")\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(training)\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))\n","user":"anonymous","dateUpdated":"2019-12-01T18:10:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])\nIntercept: 0.224563159613\n"}]},"apps":[],"jobName":"paragraph_1575223326694_-128345655","id":"20161129-010106_1160729799","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:248","dateFinished":"2019-12-01T18:11:00+0000","dateStarted":"2019-12-01T18:10:58+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=48","http://172.17.0.2:4040/jobs/job?id=49","http://172.17.0.2:4040/jobs/job?id=50","http://172.17.0.2:4040/jobs/job?id=51","http://172.17.0.2:4040/jobs/job?id=52","http://172.17.0.2:4040/jobs/job?id=53","http://172.17.0.2:4040/jobs/job?id=54","http://172.17.0.2:4040/jobs/job?id=55","http://172.17.0.2:4040/jobs/job?id=56","http://172.17.0.2:4040/jobs/job?id=57","http://172.17.0.2:4040/jobs/job?id=58","http://172.17.0.2:4040/jobs/job?id=59","http://172.17.0.2:4040/jobs/job?id=60","http://172.17.0.2:4040/jobs/job?id=61","http://172.17.0.2:4040/jobs/job?id=62","http://172.17.0.2:4040/jobs/job?id=63"],"interpreterSettingId":"spark"}}},{"text":"%md \n# Decision tree classifier\n","user":"anonymous","dateUpdated":"2019-12-01T18:11:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Decision tree classifier</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326695_1823258541","id":"20161129-010330_366107409","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:249","dateFinished":"2019-12-01T18:11:07+0000","dateStarted":"2019-12-01T18:11:07+0000"},{"text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Load the data stored in LIBSVM format as a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_libsvm_data.txt\")\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n# Automatically identify categorical features, and index them.\n# We specify maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a DecisionTree model.\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n\n# Chain indexers and tree in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\n\ntreeModel = model.stages[2]\n# summary only\nprint(treeModel)","user":"anonymous","dateUpdated":"2019-12-01T18:11:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------+--------------------+\n|prediction|indexedLabel|            features|\n+----------+------------+--------------------+\n|       1.0|         1.0|(692,[122,123,124...|\n|       1.0|         1.0|(692,[123,124,125...|\n|       1.0|         1.0|(692,[123,124,125...|\n|       1.0|         1.0|(692,[124,125,126...|\n|       1.0|         1.0|(692,[124,125,126...|\n+----------+------------+--------------------+\nonly showing top 5 rows\n\nTest Error = 0.0384615 \nDecisionTreeClassificationModel (uid=DecisionTreeClassifier_45959edfc30aed10c981) of depth 2 with 5 nodes\n"}]},"apps":[],"jobName":"paragraph_1575223326695_-2002006764","id":"20161129-010551_944142688","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:250","dateFinished":"2019-12-01T18:11:15+0000","dateStarted":"2019-12-01T18:11:09+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=64","http://172.17.0.2:4040/jobs/job?id=65","http://172.17.0.2:4040/jobs/job?id=66","http://172.17.0.2:4040/jobs/job?id=67","http://172.17.0.2:4040/jobs/job?id=68","http://172.17.0.2:4040/jobs/job?id=69","http://172.17.0.2:4040/jobs/job?id=70","http://172.17.0.2:4040/jobs/job?id=71","http://172.17.0.2:4040/jobs/job?id=72","http://172.17.0.2:4040/jobs/job?id=73","http://172.17.0.2:4040/jobs/job?id=74","http://172.17.0.2:4040/jobs/job?id=75"],"interpreterSettingId":"spark"}}},{"text":"%md\n# Regression\n\n## Random forest regression","user":"anonymous","dateUpdated":"2019-12-01T18:13:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Regression</h1>\n<h2>Random forest regression</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326696_-996738015","id":"20161129-010635_572224039","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:251","dateFinished":"2019-12-01T18:13:22+0000","dateStarted":"2019-12-01T18:13:22+0000"},{"text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Load and parse the data file, converting it to a DataFrame.\ndata = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_libsvm_data.txt\")\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =\\\n    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) = data.randomSplit([0.7, 0.3])\n\n# Train a RandomForest model.\nrf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n\n# Chain indexer and forest in a Pipeline\npipeline = Pipeline(stages=[featureIndexer, rf])\n\n# Train model.  This also runs the indexer.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"label\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = RegressionEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\nrfModel = model.stages[1]\nprint(rfModel)  # summary only","user":"anonymous","dateUpdated":"2019-12-01T18:13:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+--------------------+\n|prediction|label|            features|\n+----------+-----+--------------------+\n|       0.0|  0.0|(692,[98,99,100,1...|\n|      0.05|  0.0|(692,[121,122,123...|\n|      0.05|  0.0|(692,[122,123,148...|\n|       0.0|  0.0|(692,[123,124,125...|\n|      0.05|  0.0|(692,[124,125,126...|\n+----------+-----+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 0.165022\nRandomForestRegressionModel (uid=RandomForestRegressor_4d5f97ddd33c34f49036) with 20 trees\n"}]},"apps":[],"jobName":"paragraph_1575223326696_-226720221","id":"20161129-011540_1646360459","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:252","dateFinished":"2019-12-01T18:13:26+0000","dateStarted":"2019-12-01T18:13:24+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=76","http://172.17.0.2:4040/jobs/job?id=77","http://172.17.0.2:4040/jobs/job?id=78","http://172.17.0.2:4040/jobs/job?id=79","http://172.17.0.2:4040/jobs/job?id=80","http://172.17.0.2:4040/jobs/job?id=81","http://172.17.0.2:4040/jobs/job?id=82","http://172.17.0.2:4040/jobs/job?id=83","http://172.17.0.2:4040/jobs/job?id=84","http://172.17.0.2:4040/jobs/job?id=85","http://172.17.0.2:4040/jobs/job?id=86"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Clustering\n\n## K-means","user":"anonymous","dateUpdated":"2019-12-01T18:02:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Clustering</h1>\n<h2>K-means</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326697_181841212","id":"20161129-011600_996178656","dateCreated":"2019-12-01T18:02:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:253"},{"text":"%pyspark\n\nfrom pyspark.ml.clustering import KMeans\n\n# Loads data.\ndataset = spark.read.format(\"libsvm\").load(\"/data/mllib/sample_kmeans_data.txt\")\n\n# Trains a k-means model.\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(dataset)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors.\nwssse = model.computeCost(dataset)\nprint(\"Within Set Sum of Squared Errors = \" + str(wssse))\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n    ","user":"anonymous","dateUpdated":"2019-12-01T18:13:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Within Set Sum of Squared Errors = 0.12\nCluster Centers: \n[ 0.1  0.1  0.1]\n[ 9.1  9.1  9.1]\n"}]},"apps":[],"jobName":"paragraph_1575223326697_108338034","id":"20161129-014411_927055504","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:254","dateFinished":"2019-12-01T18:13:33+0000","dateStarted":"2019-12-01T18:13:32+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=87","http://172.17.0.2:4040/jobs/job?id=88","http://172.17.0.2:4040/jobs/job?id=89","http://172.17.0.2:4040/jobs/job?id=90","http://172.17.0.2:4040/jobs/job?id=91","http://172.17.0.2:4040/jobs/job?id=92","http://172.17.0.2:4040/jobs/job?id=93","http://172.17.0.2:4040/jobs/job?id=94","http://172.17.0.2:4040/jobs/job?id=95","http://172.17.0.2:4040/jobs/job?id=96","http://172.17.0.2:4040/jobs/job?id=97"],"interpreterSettingId":"spark"}}},{"text":"%md\n\n# Cross-Validation\n","user":"anonymous","dateUpdated":"2019-12-01T18:13:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Cross-Validation</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1575223326697_1029925583","id":"20161129-014829_2105425566","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:255","dateFinished":"2019-12-01T18:13:42+0000","dateStarted":"2019-12-01T18:13:42+0000"},{"text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Prepare training documents, which are labeled.\ntraining = spark.createDataFrame([\n    (0, \"a b c d e spark\", 1.0),\n    (1, \"b d\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0),\n    (4, \"b spark who\", 1.0),\n    (5, \"g d a y\", 0.0),\n    (6, \"spark fly\", 1.0),\n    (7, \"was mapreduce\", 0.0),\n    (8, \"e spark program\", 1.0),\n    (9, \"a e c l\", 0.0),\n    (10, \"spark compile\", 1.0),\n    (11, \"hadoop software\", 0.0)\n], [\"id\", \"text\", \"label\"])\n\n# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\nlr = LogisticRegression(maxIter=10)\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n\n# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n# This will allow us to jointly choose parameters for all Pipeline stages.\n# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n# We use a ParamGridBuilder to construct a grid of parameters to search over.\n# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nparamGrid = ParamGridBuilder() \\\n    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n    .addGrid(lr.regParam, [0.1, 0.01]) \\\n    .build()\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2)  # use 3+ folds in practice\n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(training)\n\n# Prepare test documents, which are unlabeled.\ntest = spark.createDataFrame([\n    (4, \"spark i j k\"),\n    (5, \"l m n\"),\n    (6, \"mapreduce spark\"),\n    (7, \"apache hadoop\")\n], [\"id\", \"text\"])\n\n# Make predictions on test documents. cvModel uses the best model found (lrModel).\nprediction = cvModel.transform(test)\nselected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\nfor row in selected.collect():\n    print(row)","user":"anonymous","dateUpdated":"2019-12-01T18:13:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Row(id=4, text=u'spark i j k', probability=DenseVector([0.2661, 0.7339]), prediction=1.0)\nRow(id=5, text=u'l m n', probability=DenseVector([0.9209, 0.0791]), prediction=0.0)\nRow(id=6, text=u'mapreduce spark', probability=DenseVector([0.4429, 0.5571]), prediction=1.0)\nRow(id=7, text=u'apache hadoop', probability=DenseVector([0.8584, 0.1416]), prediction=0.0)\n"}]},"apps":[],"jobName":"paragraph_1575223326698_-821764550","id":"20161129-015223_1699705288","dateCreated":"2019-12-01T18:02:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:256","dateFinished":"2019-12-01T18:14:00+0000","dateStarted":"2019-12-01T18:13:44+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=98","http://172.17.0.2:4040/jobs/job?id=99","http://172.17.0.2:4040/jobs/job?id=100","http://172.17.0.2:4040/jobs/job?id=101","http://172.17.0.2:4040/jobs/job?id=102","http://172.17.0.2:4040/jobs/job?id=103","http://172.17.0.2:4040/jobs/job?id=104","http://172.17.0.2:4040/jobs/job?id=105","http://172.17.0.2:4040/jobs/job?id=106","http://172.17.0.2:4040/jobs/job?id=107","http://172.17.0.2:4040/jobs/job?id=108","http://172.17.0.2:4040/jobs/job?id=109","http://172.17.0.2:4040/jobs/job?id=110","http://172.17.0.2:4040/jobs/job?id=111","http://172.17.0.2:4040/jobs/job?id=112","http://172.17.0.2:4040/jobs/job?id=113","http://172.17.0.2:4040/jobs/job?id=114","http://172.17.0.2:4040/jobs/job?id=115","http://172.17.0.2:4040/jobs/job?id=116","http://172.17.0.2:4040/jobs/job?id=117","http://172.17.0.2:4040/jobs/job?id=118","http://172.17.0.2:4040/jobs/job?id=119","http://172.17.0.2:4040/jobs/job?id=120","http://172.17.0.2:4040/jobs/job?id=121","http://172.17.0.2:4040/jobs/job?id=122","http://172.17.0.2:4040/jobs/job?id=123","http://172.17.0.2:4040/jobs/job?id=124","http://172.17.0.2:4040/jobs/job?id=125","http://172.17.0.2:4040/jobs/job?id=126","http://172.17.0.2:4040/jobs/job?id=127","http://172.17.0.2:4040/jobs/job?id=128","http://172.17.0.2:4040/jobs/job?id=129","http://172.17.0.2:4040/jobs/job?id=130","http://172.17.0.2:4040/jobs/job?id=131","http://172.17.0.2:4040/jobs/job?id=132","http://172.17.0.2:4040/jobs/job?id=133","http://172.17.0.2:4040/jobs/job?id=134","http://172.17.0.2:4040/jobs/job?id=135","http://172.17.0.2:4040/jobs/job?id=136","http://172.17.0.2:4040/jobs/job?id=137","http://172.17.0.2:4040/jobs/job?id=138","http://172.17.0.2:4040/jobs/job?id=139","http://172.17.0.2:4040/jobs/job?id=140","http://172.17.0.2:4040/jobs/job?id=141","http://172.17.0.2:4040/jobs/job?id=142","http://172.17.0.2:4040/jobs/job?id=143","http://172.17.0.2:4040/jobs/job?id=144","http://172.17.0.2:4040/jobs/job?id=145","http://172.17.0.2:4040/jobs/job?id=146","http://172.17.0.2:4040/jobs/job?id=147","http://172.17.0.2:4040/jobs/job?id=148","http://172.17.0.2:4040/jobs/job?id=149","http://172.17.0.2:4040/jobs/job?id=150","http://172.17.0.2:4040/jobs/job?id=151","http://172.17.0.2:4040/jobs/job?id=152","http://172.17.0.2:4040/jobs/job?id=153","http://172.17.0.2:4040/jobs/job?id=154","http://172.17.0.2:4040/jobs/job?id=155","http://172.17.0.2:4040/jobs/job?id=156","http://172.17.0.2:4040/jobs/job?id=157","http://172.17.0.2:4040/jobs/job?id=158","http://172.17.0.2:4040/jobs/job?id=159","http://172.17.0.2:4040/jobs/job?id=160","http://172.17.0.2:4040/jobs/job?id=161","http://172.17.0.2:4040/jobs/job?id=162","http://172.17.0.2:4040/jobs/job?id=163","http://172.17.0.2:4040/jobs/job?id=164","http://172.17.0.2:4040/jobs/job?id=165","http://172.17.0.2:4040/jobs/job?id=166","http://172.17.0.2:4040/jobs/job?id=167","http://172.17.0.2:4040/jobs/job?id=168","http://172.17.0.2:4040/jobs/job?id=169","http://172.17.0.2:4040/jobs/job?id=170","http://172.17.0.2:4040/jobs/job?id=171","http://172.17.0.2:4040/jobs/job?id=172","http://172.17.0.2:4040/jobs/job?id=173","http://172.17.0.2:4040/jobs/job?id=174","http://172.17.0.2:4040/jobs/job?id=175","http://172.17.0.2:4040/jobs/job?id=176","http://172.17.0.2:4040/jobs/job?id=177","http://172.17.0.2:4040/jobs/job?id=178","http://172.17.0.2:4040/jobs/job?id=179","http://172.17.0.2:4040/jobs/job?id=180","http://172.17.0.2:4040/jobs/job?id=181","http://172.17.0.2:4040/jobs/job?id=182","http://172.17.0.2:4040/jobs/job?id=183","http://172.17.0.2:4040/jobs/job?id=184","http://172.17.0.2:4040/jobs/job?id=185","http://172.17.0.2:4040/jobs/job?id=186","http://172.17.0.2:4040/jobs/job?id=187","http://172.17.0.2:4040/jobs/job?id=188","http://172.17.0.2:4040/jobs/job?id=189","http://172.17.0.2:4040/jobs/job?id=190","http://172.17.0.2:4040/jobs/job?id=191","http://172.17.0.2:4040/jobs/job?id=192","http://172.17.0.2:4040/jobs/job?id=193","http://172.17.0.2:4040/jobs/job?id=194","http://172.17.0.2:4040/jobs/job?id=195","http://172.17.0.2:4040/jobs/job?id=196","http://172.17.0.2:4040/jobs/job?id=197","http://172.17.0.2:4040/jobs/job?id=198","http://172.17.0.2:4040/jobs/job?id=199","http://172.17.0.2:4040/jobs/job?id=200","http://172.17.0.2:4040/jobs/job?id=201","http://172.17.0.2:4040/jobs/job?id=202","http://172.17.0.2:4040/jobs/job?id=203","http://172.17.0.2:4040/jobs/job?id=204","http://172.17.0.2:4040/jobs/job?id=205","http://172.17.0.2:4040/jobs/job?id=206","http://172.17.0.2:4040/jobs/job?id=207","http://172.17.0.2:4040/jobs/job?id=208","http://172.17.0.2:4040/jobs/job?id=209","http://172.17.0.2:4040/jobs/job?id=210","http://172.17.0.2:4040/jobs/job?id=211","http://172.17.0.2:4040/jobs/job?id=212","http://172.17.0.2:4040/jobs/job?id=213","http://172.17.0.2:4040/jobs/job?id=214","http://172.17.0.2:4040/jobs/job?id=215","http://172.17.0.2:4040/jobs/job?id=216","http://172.17.0.2:4040/jobs/job?id=217","http://172.17.0.2:4040/jobs/job?id=218","http://172.17.0.2:4040/jobs/job?id=219","http://172.17.0.2:4040/jobs/job?id=220","http://172.17.0.2:4040/jobs/job?id=221","http://172.17.0.2:4040/jobs/job?id=222","http://172.17.0.2:4040/jobs/job?id=223","http://172.17.0.2:4040/jobs/job?id=224","http://172.17.0.2:4040/jobs/job?id=225","http://172.17.0.2:4040/jobs/job?id=226","http://172.17.0.2:4040/jobs/job?id=227","http://172.17.0.2:4040/jobs/job?id=228","http://172.17.0.2:4040/jobs/job?id=229","http://172.17.0.2:4040/jobs/job?id=230","http://172.17.0.2:4040/jobs/job?id=231","http://172.17.0.2:4040/jobs/job?id=232","http://172.17.0.2:4040/jobs/job?id=233","http://172.17.0.2:4040/jobs/job?id=234","http://172.17.0.2:4040/jobs/job?id=235","http://172.17.0.2:4040/jobs/job?id=236","http://172.17.0.2:4040/jobs/job?id=237","http://172.17.0.2:4040/jobs/job?id=238","http://172.17.0.2:4040/jobs/job?id=239","http://172.17.0.2:4040/jobs/job?id=240","http://172.17.0.2:4040/jobs/job?id=241","http://172.17.0.2:4040/jobs/job?id=242","http://172.17.0.2:4040/jobs/job?id=243","http://172.17.0.2:4040/jobs/job?id=244","http://172.17.0.2:4040/jobs/job?id=245","http://172.17.0.2:4040/jobs/job?id=246","http://172.17.0.2:4040/jobs/job?id=247","http://172.17.0.2:4040/jobs/job?id=248","http://172.17.0.2:4040/jobs/job?id=249","http://172.17.0.2:4040/jobs/job?id=250","http://172.17.0.2:4040/jobs/job?id=251","http://172.17.0.2:4040/jobs/job?id=252","http://172.17.0.2:4040/jobs/job?id=253","http://172.17.0.2:4040/jobs/job?id=254","http://172.17.0.2:4040/jobs/job?id=255","http://172.17.0.2:4040/jobs/job?id=256","http://172.17.0.2:4040/jobs/job?id=257","http://172.17.0.2:4040/jobs/job?id=258","http://172.17.0.2:4040/jobs/job?id=259","http://172.17.0.2:4040/jobs/job?id=260","http://172.17.0.2:4040/jobs/job?id=261","http://172.17.0.2:4040/jobs/job?id=262","http://172.17.0.2:4040/jobs/job?id=263","http://172.17.0.2:4040/jobs/job?id=264","http://172.17.0.2:4040/jobs/job?id=265","http://172.17.0.2:4040/jobs/job?id=266","http://172.17.0.2:4040/jobs/job?id=267","http://172.17.0.2:4040/jobs/job?id=268","http://172.17.0.2:4040/jobs/job?id=269","http://172.17.0.2:4040/jobs/job?id=270","http://172.17.0.2:4040/jobs/job?id=271","http://172.17.0.2:4040/jobs/job?id=272","http://172.17.0.2:4040/jobs/job?id=273","http://172.17.0.2:4040/jobs/job?id=274","http://172.17.0.2:4040/jobs/job?id=275","http://172.17.0.2:4040/jobs/job?id=276","http://172.17.0.2:4040/jobs/job?id=277","http://172.17.0.2:4040/jobs/job?id=278","http://172.17.0.2:4040/jobs/job?id=279","http://172.17.0.2:4040/jobs/job?id=280","http://172.17.0.2:4040/jobs/job?id=281","http://172.17.0.2:4040/jobs/job?id=282","http://172.17.0.2:4040/jobs/job?id=283","http://172.17.0.2:4040/jobs/job?id=284","http://172.17.0.2:4040/jobs/job?id=285","http://172.17.0.2:4040/jobs/job?id=286","http://172.17.0.2:4040/jobs/job?id=287","http://172.17.0.2:4040/jobs/job?id=288","http://172.17.0.2:4040/jobs/job?id=289","http://172.17.0.2:4040/jobs/job?id=290","http://172.17.0.2:4040/jobs/job?id=291","http://172.17.0.2:4040/jobs/job?id=292"],"interpreterSettingId":"spark"}}},{"text":"","user":"anonymous","dateUpdated":"2019-12-01T18:02:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1575223326699_822594284","id":"20161129-015302_1115287316","dateCreated":"2019-12-01T18:02:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:257"}],"name":"MLlib-Lab-Session","id":"2ETF8UU14","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}