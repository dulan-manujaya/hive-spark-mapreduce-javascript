{"paragraphs":[{"text":"%pyspark\n\nimport libs\n\nfrom pyspark.sql import SparkSession, Row, functions, types\nfrom pyspark.sql.functions import udf\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler, StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\nfrom pyspark.rdd import RDD\nfrom pyspark.sql.functions import col\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import when\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import lit\nfrom datetime import date\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import *\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\nfrom pyspark.rdd import RDD\n\n\ndf = spark.read.csv(\"/data/listings.csv\", header=\"true\", inferSchema=\"true\")\ncolmns=['id','latitude','longitude','neighbourhood_group']\ndf_training = df.select(colmns)\ndf_training = df_training.na.drop()\ndf_training = df_training.withColumn(\"longitude\", df_training[\"longitude\"].cast(DoubleType()))\ndf_training.printSchema()\n\ndf_training.createOrReplaceTempView(\"cols\")\n\ndf1_training = spark.sql(\"select neighbourhood_group, count(id) id_count from cols group by neighbourhood_group order by id_count DESC\")\ndf1_training.show()\n\n#ignore minimul popular regions\ndef valueToInt(value):\n    if value=='Central Region': return 5\n    elif value=='East Region': return 4\n    elif value=='North Region': return 3\n    elif value=='North-East Region': return 2\n    elif value=='West Region': return  1\n    else: return 6\n   \nudfValueToInt = udf(valueToInt, IntegerType())\ndf1_training = df_training.withColumn(\"label_column\", udfValueToInt(\"neighbourhood_group\"))\n\ndf2_training = df1_training.filter(df1_training['label_column'] < 6)\ndf2_training.show()\n\ndef vectorAssembling(df,impFeatures):\nprint(\" vectorAssembling start.. \")\nassembler = VectorAssembler(inputCols=impFeatures,outputCol=\"features\")\ndf_new=assembler.transform(df)\nprint(\"vectorAssembling done.\")\nreturn df_new\ninputs=['latitude','longitude']\nimpdf = df2_training.select(impCols).show()\n# Create one feature vector column using latitude and longitude\ndtf=vectorAssembling(df2_training,inputs)\ndtf.show()\n\nfinalized_data = dtf.select('label_column', 'features')\nfrom pyspark.ml.feature import StandardScaler\nscaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\nscalerModel = scaler.fit(finalized_data)\nclassiFinalData = scalerModel.transform(finalized_data)\n\n# Split the data into training and test sets (20% held out for testing)\n(trainingData, testData) = classiFinalData.randomSplit([0.8, 0.2])\n\ntrainingData.show()\n\n\ndef TrainingModel(trainDF):\nprint(\"TrainingModel started\")\nlr = LogisticRegression(labelCol=\"label_column\", featuresCol=\"scaledFeatures\",elasticNetParam=0.8, family=\"multinomial\",maxIter=100)\nmodel=lr.fit(trainDF)\nprint(\"TrainingModel completed\")\nreturn model\n\nmodel = TrainingModel(trainingData)\n\n# Get predictions.\npredictions = model.transform(testData)\n\n# Predicted data\npredictions.select(\"prediction\", \"label_column\", \"features\").show(100)\n\n\n\n\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\nfrom pyspark.rdd import RDD\n\nevaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label_column\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nevaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label_column\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nrecall = evaluatorRecall.evaluate(predictions)\nprecision = evaluatorPrecision.evaluate(predictions)\nprint(\"Recall %s\" % recall)\nprint(\"Precision %s\" % precision)","user":"anonymous","dateUpdated":"2020-01-12T17:28:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578849934718_2076310984","id":"20200112-172534_227554813","dateCreated":"2020-01-12T17:25:34+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2512","dateFinished":"2020-01-12T17:28:04+0000","dateStarted":"2020-01-12T17:28:04+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-ece10847db9b>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    print (\" vectorAssembling start.. \")\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"}]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-12T17:26:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1578850013976_-219462479","id":"20200112-172653_457302834","dateCreated":"2020-01-12T17:26:53+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2595"}],"name":"Machine Learning","id":"2EXFYP25W","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}