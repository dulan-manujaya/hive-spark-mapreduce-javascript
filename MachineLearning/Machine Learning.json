{"paragraphs":[{"text":"%pyspark\n\ndf = spark.read.csv(\"/data/listings.csv\", header=\"true\", inferSchema=\"true\")\n\nfrom pyspark.sql import SparkSession, Row, functions, types\nfrom pyspark.sql.functions import udf\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler, StringIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\nfrom pyspark.rdd import RDD\nfrom pyspark.sql.functions import col\nimport pandas as pd \nimport numpy as np \nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import when\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import lit\nfrom datetime import date\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import *\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\nfrom pyspark.rdd import RDD\n\n#3.Data Preprocessing\n#COlumns need tor the model tarining and make a new dataframe \nfeatures=['id','latitude','longitude','neighbourhood_group']\ndf_training = df.select(features)\n#df_training.show()\ndf_training = df_training.na.drop()\ndf_training = df_training.withColumn(\"longitude\", df_training[\"longitude\"].cast(DoubleType()))\n\n\ndf_training.createOrReplaceTempView(\"listings_view\")\n\ndf1_training = spark.sql(\"select  neighbourhood_group, count(id) id_count from listings_view group by neighbourhood_group order by id_count DESC\")\n#df1_training.show()\n#Ignore the neighbourhood group in which they have count less than 10\n\ndef valueToInt(value):\n    if value=='Central Region': return 1\n    elif value=='West Region': return  2\n    elif value=='East Region': return 3\n    elif value=='North-East Region': return 4\n    elif value=='North Region': return 5\n    else: return 6\n    \nudfValueToInt = udf(valueToInt, IntegerType())\ndf1_training = df_training.withColumn(\"label_column\", udfValueToInt(\"neighbourhood_group\"))\ndf2_training = df1_training.filter(df1_training['label_column'] < 6)\n\n\ndef vectAssembler(df,impFeatures):\n\t\n\tassembler = VectorAssembler(inputCols=impFeatures,outputCol=\"features\")\n\tdf_new=assembler.transform(df)\n\t\n\treturn df_new\nimpFeatures=['latitude','longitude']\n\n# Create a feature vector column using latitude and longitude \ndtf=vectAssembler(df2_training,impFeatures)\n\nfinalized_data = dtf.select('label_column', 'features')\nfrom pyspark.ml.feature import StandardScaler\nscaler = StandardScaler(inputCol='features', outputCol='scaledFeatures', withStd=True, withMean=True)\nscalerModel = scaler.fit(finalized_data)\nclassiFinalData = scalerModel.transform(finalized_data)\n\n# Splitting data into training and test sets (30% for testing)\n(trainingData, testData) = classiFinalData.randomSplit([0.8, 0.2])\n\n\n\ndef LRTraining(trainDF):\n\t\n\tlr = LogisticRegression(labelCol=\"label_column\", featuresCol=\"scaledFeatures\",elasticNetParam=0.8, family=\"multinomial\",maxIter=120)\n\tmodel=lr.fit(trainDF)\n\n\treturn model \n\t\nmodel = LRTraining(trainingData)\n\n# Get predictions.\npredictions = model.transform(testData)\n\n# Predicted data \npredictions.select(\"prediction\", \"label_column\", \"features\").show()\n\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nfrom pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\nfrom pyspark.rdd import RDD\n\nevaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label_column\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nevaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label_column\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nrecall = evaluatorRecall.evaluate(predictions)\nprecision = evaluatorPrecision.evaluate(predictions)\nprint(\"Recall %s\" % recall)\nprint(\"Precision %s\" % precision)\n","user":"anonymous","dateUpdated":"2020-01-18T09:23:25+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------+-------------------+\n|prediction|label_column|           features|\n+----------+------------+-------------------+\n|       1.0|           1|[1.24387,103.84246]|\n|       1.0|           1|[1.24526,103.83999]|\n|       1.0|           1|[1.24847,103.82389]|\n|       1.0|           1|[1.25284,103.82225]|\n|       1.0|           1|  [1.25602,103.821]|\n|       1.0|           1|[1.25639,103.82302]|\n|       1.0|           1| [1.26504,103.8146]|\n|       1.0|           1|[1.26513,103.81722]|\n|       1.0|           1|[1.26582,103.81531]|\n|       1.0|           1| [1.2667,103.81127]|\n|       1.0|           1|[1.26814,103.81507]|\n|       1.0|           1|[1.27112,103.82411]|\n|       1.0|           1|[1.27173,103.82232]|\n|       1.0|           1|[1.27229,103.80976]|\n|       1.0|           1|[1.27234,103.83224]|\n|       1.0|           1|[1.27237,103.83233]|\n|       1.0|           1|[1.27262,103.83408]|\n|       1.0|           1| [1.27291,103.8338]|\n|       1.0|           1|[1.27316,103.84648]|\n|       1.0|           1|[1.27331,103.84422]|\n+----------+------------+-------------------+\nonly showing top 20 rows\n\nRecall 0.983397190294\nPrecision 0.983927885853\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.2:4040/jobs/job?id=311","http://172.17.0.2:4040/jobs/job?id=312","http://172.17.0.2:4040/jobs/job?id=313","http://172.17.0.2:4040/jobs/job?id=314","http://172.17.0.2:4040/jobs/job?id=315","http://172.17.0.2:4040/jobs/job?id=316","http://172.17.0.2:4040/jobs/job?id=317","http://172.17.0.2:4040/jobs/job?id=318","http://172.17.0.2:4040/jobs/job?id=319","http://172.17.0.2:4040/jobs/job?id=320","http://172.17.0.2:4040/jobs/job?id=321","http://172.17.0.2:4040/jobs/job?id=322","http://172.17.0.2:4040/jobs/job?id=323","http://172.17.0.2:4040/jobs/job?id=324","http://172.17.0.2:4040/jobs/job?id=325","http://172.17.0.2:4040/jobs/job?id=326","http://172.17.0.2:4040/jobs/job?id=327","http://172.17.0.2:4040/jobs/job?id=328","http://172.17.0.2:4040/jobs/job?id=329","http://172.17.0.2:4040/jobs/job?id=330","http://172.17.0.2:4040/jobs/job?id=331","http://172.17.0.2:4040/jobs/job?id=332","http://172.17.0.2:4040/jobs/job?id=333","http://172.17.0.2:4040/jobs/job?id=334","http://172.17.0.2:4040/jobs/job?id=335","http://172.17.0.2:4040/jobs/job?id=336","http://172.17.0.2:4040/jobs/job?id=337","http://172.17.0.2:4040/jobs/job?id=338","http://172.17.0.2:4040/jobs/job?id=339","http://172.17.0.2:4040/jobs/job?id=340","http://172.17.0.2:4040/jobs/job?id=341","http://172.17.0.2:4040/jobs/job?id=342","http://172.17.0.2:4040/jobs/job?id=343","http://172.17.0.2:4040/jobs/job?id=344","http://172.17.0.2:4040/jobs/job?id=345","http://172.17.0.2:4040/jobs/job?id=346","http://172.17.0.2:4040/jobs/job?id=347","http://172.17.0.2:4040/jobs/job?id=348","http://172.17.0.2:4040/jobs/job?id=349","http://172.17.0.2:4040/jobs/job?id=350","http://172.17.0.2:4040/jobs/job?id=351","http://172.17.0.2:4040/jobs/job?id=352","http://172.17.0.2:4040/jobs/job?id=353","http://172.17.0.2:4040/jobs/job?id=354","http://172.17.0.2:4040/jobs/job?id=355","http://172.17.0.2:4040/jobs/job?id=356","http://172.17.0.2:4040/jobs/job?id=357","http://172.17.0.2:4040/jobs/job?id=358","http://172.17.0.2:4040/jobs/job?id=359","http://172.17.0.2:4040/jobs/job?id=360","http://172.17.0.2:4040/jobs/job?id=361","http://172.17.0.2:4040/jobs/job?id=362","http://172.17.0.2:4040/jobs/job?id=363","http://172.17.0.2:4040/jobs/job?id=364","http://172.17.0.2:4040/jobs/job?id=365","http://172.17.0.2:4040/jobs/job?id=366","http://172.17.0.2:4040/jobs/job?id=367","http://172.17.0.2:4040/jobs/job?id=368","http://172.17.0.2:4040/jobs/job?id=369","http://172.17.0.2:4040/jobs/job?id=370","http://172.17.0.2:4040/jobs/job?id=371","http://172.17.0.2:4040/jobs/job?id=372","http://172.17.0.2:4040/jobs/job?id=373","http://172.17.0.2:4040/jobs/job?id=374","http://172.17.0.2:4040/jobs/job?id=375","http://172.17.0.2:4040/jobs/job?id=376","http://172.17.0.2:4040/jobs/job?id=377","http://172.17.0.2:4040/jobs/job?id=378","http://172.17.0.2:4040/jobs/job?id=379","http://172.17.0.2:4040/jobs/job?id=380","http://172.17.0.2:4040/jobs/job?id=381","http://172.17.0.2:4040/jobs/job?id=382","http://172.17.0.2:4040/jobs/job?id=383","http://172.17.0.2:4040/jobs/job?id=384","http://172.17.0.2:4040/jobs/job?id=385","http://172.17.0.2:4040/jobs/job?id=386","http://172.17.0.2:4040/jobs/job?id=387","http://172.17.0.2:4040/jobs/job?id=388","http://172.17.0.2:4040/jobs/job?id=389","http://172.17.0.2:4040/jobs/job?id=390","http://172.17.0.2:4040/jobs/job?id=391","http://172.17.0.2:4040/jobs/job?id=392","http://172.17.0.2:4040/jobs/job?id=393","http://172.17.0.2:4040/jobs/job?id=394","http://172.17.0.2:4040/jobs/job?id=395","http://172.17.0.2:4040/jobs/job?id=396","http://172.17.0.2:4040/jobs/job?id=397","http://172.17.0.2:4040/jobs/job?id=398","http://172.17.0.2:4040/jobs/job?id=399","http://172.17.0.2:4040/jobs/job?id=400","http://172.17.0.2:4040/jobs/job?id=401","http://172.17.0.2:4040/jobs/job?id=402","http://172.17.0.2:4040/jobs/job?id=403","http://172.17.0.2:4040/jobs/job?id=404","http://172.17.0.2:4040/jobs/job?id=405","http://172.17.0.2:4040/jobs/job?id=406","http://172.17.0.2:4040/jobs/job?id=407","http://172.17.0.2:4040/jobs/job?id=408","http://172.17.0.2:4040/jobs/job?id=409","http://172.17.0.2:4040/jobs/job?id=410","http://172.17.0.2:4040/jobs/job?id=411"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1579338916610_-699944743","id":"20200112-172653_457302834","dateCreated":"2020-01-18T09:15:16+0000","dateStarted":"2020-01-18T09:23:25+0000","dateFinished":"2020-01-18T09:23:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1827"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-01-18T09:23:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579339088837_854647015","id":"20200118-091808_2076451229","dateCreated":"2020-01-18T09:18:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1828"}],"name":"Machine Learning","id":"2EXKJ77VS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}